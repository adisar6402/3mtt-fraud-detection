{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a6b828b",
      "metadata": {
        "id": "6a6b828b"
      },
      "source": [
        "### **Deploying a Machine Learning Model as a REST API with Real-Time Streaming**\n",
        "\n",
        "#### **Introduction**\n",
        "You are a machine learning engineer at a fintech startup. Your team has developed a fraud detection model that predicts whether a transaction is fraudulent or not. Your task is to deploy this model as a REST API using **FastAPI**, containerize it with **Docker**, and set up real-time streaming for live predictions using **Kafka**. This project will allow your team to integrate the model into your live transaction processing system.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Setting Up the Environment**\n",
        "**Story:**  \n",
        "You start by setting up your development environment. You decide to use **FastAPI** for building the REST API because of its speed and ease of use. You also plan to use **Docker** to containerize the application for easy deployment and scalability. Finally, you set up **Kafka** to handle real-time streaming of transaction data for live predictions.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Install Python and create a virtual environment.\n",
        "2. Install FastAPI, Uvicorn, and other required libraries (e.g., `pydantic`, `scikit-learn`, `kafka-python`).\n",
        "3. Set up a local Kafka server using Docker (use the `confluentinc/cp-kafka` image).\n",
        "4. Verify that Kafka is running by creating a test topic and producing/consuming messages.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Building the Machine Learning Model**\n",
        "**Story:**  \n",
        "You already have a trained fraud detection model, but for this project, you decide to train a simple logistic regression model on a sample dataset (e.g., the [Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud)). You save the trained model as a `.pkl` file for later use.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Load the dataset and preprocess it (e.g., handle missing values, scale features).\n",
        "2. Train a logistic regression model using scikit-learn.\n",
        "3. Save the trained model as a `.pkl` file using `joblib` or `pickle`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Creating the REST API with FastAPI**\n",
        "**Story:**  \n",
        "You build a REST API using FastAPI that exposes an endpoint for making predictions. The API takes transaction data as input, loads the trained model, and returns the prediction (fraudulent or not).\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create a FastAPI application with a `/predict` endpoint.\n",
        "2. Load the trained model from the `.pkl` file when the API starts.\n",
        "3. Define a Pydantic model for the input data (e.g., transaction amount, timestamp, features).\n",
        "4. Implement the prediction logic in the `/predict` endpoint.\n",
        "5. Test the API locally using Uvicorn and sample transaction data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Containerizing the Application with Docker**\n",
        "**Story:**  \n",
        "To make the API easy to deploy and scale, you containerize it using Docker. You create a `Dockerfile` that sets up the environment, installs dependencies, and runs the FastAPI application.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create a `Dockerfile` that:\n",
        "   - Uses a base Python image (e.g., `python:3.9-slim`).\n",
        "   - Installs dependencies from a `requirements.txt` file.\n",
        "   - Copies the FastAPI application and model file into the container.\n",
        "   - Exposes the API on port 8000.\n",
        "2. Build the Docker image and run it locally.\n",
        "3. Test the API inside the Docker container using sample transaction data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Setting Up Real-Time Streaming with Kafka**\n",
        "**Story:**  \n",
        "To enable real-time predictions, you set up Kafka to stream transaction data to the API. The API consumes messages from a Kafka topic, makes predictions, and writes the results to another Kafka topic.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create two Kafka topics: `transactions` (for incoming transaction data) and `predictions` (for prediction results).\n",
        "2. Modify the FastAPI application to include a Kafka consumer and producer:\n",
        "   - The consumer reads transaction data from the `transactions` topic.\n",
        "   - The producer writes predictions to the `predictions` topic.\n",
        "3. Test the real-time streaming setup by producing sample transaction data to the `transactions` topic and consuming predictions from the `predictions` topic.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Deploying the Application**\n",
        "**Story:**  \n",
        "You deploy the Dockerized FastAPI application and Kafka setup to a cloud platform (e.g., AWS, Azure, or GCP) or run it locally for testing. You ensure that the API and Kafka are working together seamlessly to provide real-time predictions.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Push the Docker image to a container registry (e.g., Docker Hub, Azure Container Registry).\n",
        "2. Deploy the Docker container to a cloud service (e.g., AWS ECS, Azure Container Instances) or run it locally using Docker Compose.\n",
        "3. Verify that the API and Kafka are working together by streaming live transaction data and checking the predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 7: Testing and Monitoring**\n",
        "**Story:**  \n",
        "You test the entire system end-to-end to ensure it works as expected. You also set up basic monitoring to track the performance of the API and Kafka.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Test the system by streaming a large batch of transaction data and verifying the predictions.\n",
        "2. Set up logging in the FastAPI application to track incoming requests and predictions.\n",
        "3. Use a tool like **Prometheus** or **Grafana** to monitor the API's performance and Kafka's message throughput.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "**Story:**  \n",
        "You‚Äôve successfully deployed a machine learning model as a REST API using FastAPI, containerized it with Docker, and set up real-time streaming for live predictions using Kafka. This system can now be integrated into your live transaction processing pipeline to detect fraudulent transactions in real time.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Document the setup process and share it with your team.\n",
        "2. Terminate any cloud resources (if used) to avoid unnecessary costs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Bonus Challenge**\n",
        "- Add authentication to the FastAPI endpoint using OAuth2 or API keys.\n",
        "- Use **Kafka Streams** or **KSQL** to perform additional real-time processing on the predictions (e.g., aggregating fraud rates by hour).\n",
        "- Deploy the system on a Kubernetes cluster for better scalability and management.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Technologies Used**\n",
        "1. **FastAPI**: For building the REST API.\n",
        "2. **Docker**: For containerizing the application.\n",
        "3. **Kafka**: For real-time streaming of transaction data.\n",
        "4. **Scikit-learn**: For training the machine learning model.\n",
        "5. **Uvicorn**: For running the FastAPI application.\n",
        "6. **Pydantic**: For validating input data.\n",
        "7. **Kafka-Python**: For interacting with Kafka in Python.\n",
        "\n",
        "---\n",
        "\n",
        "This project provides a hands-on experience with deploying machine learning models, building REST APIs, containerization, and real-time streaming. It‚Äôs a great way to learn modern MLOps practices and tools."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ 3MTT AI/ML Final Project ‚Äî Fraud Detection REST API with Streaming\n",
        "**üë®üèæ‚Äçüíª Submitted by:** Abdulrahman Adisa Amuda  \n",
        "**üìÅ Module:** 7 - Machine Learning Deployment  \n",
        "**üéØ Goal:** Deploy a fraud detection ML model as a REST API using FastAPI, Docker, and Kafka-style streaming simulation.\n",
        "\n",
        "---\n",
        "\n",
        "### üìò Project Overview\n",
        "\n",
        "As a machine learning engineer at a fintech startup, I was tasked to deploy a fraud detection model with:\n",
        "\n",
        "- ‚úÖ **FastAPI** to expose predictions via a `/predict` endpoint  \n",
        "- ‚úÖ **Docker** to containerize the app for deployment  \n",
        "- ‚úÖ **Kafka** (or simulated Kafka) to stream real-time transaction data  \n",
        "- ‚úÖ **Monitoring** (described in markdown) for real-world use\n",
        "\n",
        "Due to Colab limitations, real server-based components (FastAPI/Docker/Kafka) are **written as real code** but **simulated locally** or provided as **standalone files** for external use.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Tools & Technologies Used\n",
        "\n",
        "- **scikit-learn** for model training  \n",
        "- **FastAPI** (code written but not executed in Colab)  \n",
        "- **Kafka-style simulation** using Python loops  \n",
        "- **Dockerfile** and `requirements.txt` included  \n",
        "- **Joblib** for saving model  \n",
        "- **Colab notebook** for submission\n",
        "\n",
        "---\n",
        "\n",
        "üìé *This notebook includes both runnable code and production-ready files.*"
      ],
      "metadata": {
        "id": "ZeImYqdqPCXr"
      },
      "id": "ZeImYqdqPCXr"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Install Dependencies\n",
        "!pip install -q scikit-learn pandas joblib"
      ],
      "metadata": {
        "id": "OG42xq5gPM-K"
      },
      "id": "OG42xq5gPM-K",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgChrDZvgnr5"
      },
      "id": "qgChrDZvgnr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 2: Build and Save the Fraud Detection ML Model\n",
        "\n",
        "In this step, i simulate the credit card fraud dataset using synthetic data with features like:\n",
        "\n",
        "- `age`\n",
        "- `salary`\n",
        "- `transaction_amount`\n",
        "\n",
        "I train a logistic regression classifier and save:\n",
        "\n",
        "- `fraud_model.pkl` ‚Üí Trained model\n",
        "- `scaler.pkl` ‚Üí Feature scaler (StandardScaler)\n",
        "\n",
        "These files will later be loaded inside the REST API."
      ],
      "metadata": {
        "id": "hwqj0H0VPZ8Z"
      },
      "id": "hwqj0H0VPZ8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 2: Import Libraries & Build Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ‚úÖ Simulate a Mini Dataset (you can also use Kaggle creditcard.csv offline)\n",
        "data = pd.DataFrame({\n",
        "    'age': [25, 45, 52, 37, 28, 33, 48],\n",
        "    'salary': [30000, 50000, 70000, 42000, 25000, 39000, 60000],\n",
        "    'transaction_amount': [1000, 3000, 5000, 2300, 1200, 1800, 4100],\n",
        "    'is_fraud': [0, 1, 1, 0, 0, 0, 1]\n",
        "})\n",
        "\n",
        "X = data[['age', 'salary', 'transaction_amount']]\n",
        "y = data['is_fraud']\n",
        "\n",
        "# ‚úÖ Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ‚úÖ Train Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# ‚úÖ Save Model and Scaler\n",
        "joblib.dump(model, 'fraud_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\"‚úÖ Model and Scaler saved as fraud_model.pkl and scaler.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82mlhLsOPkAH",
        "outputId": "e227efdc-212f-4c4f-b5ca-b44a32d960c4"
      },
      "id": "82mlhLsOPkAH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and Scaler saved as fraud_model.pkl and scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 3: Simulated FastAPI Prediction Logic\n",
        "In this step, i simulate the logic behind a FastAPI /predict endpoint by writing a Python function that:\n",
        "\n",
        "Loads the saved model and scaler\n",
        "Accepts user inputs (age, salary, transaction_amount)\n",
        "Returns the predicted result: \"Fraudulent\" or \"Legit\"\n",
        "This prepares us for integration with a real FastAPI application later."
      ],
      "metadata": {
        "id": "VbcCYCIePzdu"
      },
      "id": "VbcCYCIePzdu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 3: Define the Simulated /predict Endpoint Logic\n",
        "def predict_transaction(age, salary, transaction_amount):\n",
        "    # Load the model and scaler\n",
        "    model = joblib.load('fraud_model.pkl')\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "    # Create input DataFrame\n",
        "    input_df = pd.DataFrame([{\n",
        "        'age': age,\n",
        "        'salary': salary,\n",
        "        'transaction_amount': transaction_amount\n",
        "    }])\n",
        "\n",
        "    # Scale input and predict\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "\n",
        "    return \"Fraudulent\" if prediction == 1 else \"Legit\""
      ],
      "metadata": {
        "id": "5WOwrfbzQNq6"
      },
      "id": "5WOwrfbzQNq6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 4: Testing API Logic + Simulated Real-Time Streaming\n",
        "In this step:\n",
        "\n",
        "I test the predict_transaction() function with a sample input.\n",
        "Then simulate a real-time stream of transactions (like Kafka) using a Python loop.\n",
        "This gives a feel for how real-time fraud prediction would look before implementing Kafka."
      ],
      "metadata": {
        "id": "EDEElqU0QT8j"
      },
      "id": "EDEElqU0QT8j"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Test the API with one transaction\n",
        "test_input = {\"age\": 45, \"salary\": 50000, \"transaction_amount\": 3000}\n",
        "result = predict_transaction(**test_input)\n",
        "print(f\"üß™ Single Transaction Test ‚Üí {test_input} ‚Üí üß† Prediction: {result}\")\n",
        "\n",
        "# ‚úÖ Simulated Kafka-style Real-Time Streaming\n",
        "import time\n",
        "\n",
        "print(\"\\nüö¶ Simulating Real-Time Transaction Stream:\\n\")\n",
        "sample_stream = [\n",
        "    {\"age\": 28, \"salary\": 25000, \"transaction_amount\": 1200},\n",
        "    {\"age\": 52, \"salary\": 70000, \"transaction_amount\": 5000},\n",
        "    {\"age\": 33, \"salary\": 39000, \"transaction_amount\": 1800},\n",
        "]\n",
        "\n",
        "for txn in sample_stream:\n",
        "    pred = predict_transaction(**txn)\n",
        "    print(f\"‚û°Ô∏è {txn} ‚Üí üß† Prediction: {pred}\")\n",
        "    time.sleep(1)  # Simulate delay between transactions\n",
        "\n",
        "print(\"\\n‚úÖ Real-Time Streaming Simulation Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1IL7lPxQXo3",
        "outputId": "6dc03054-f456-4749-c753-2ced5a5ea9a6"
      },
      "id": "z1IL7lPxQXo3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Single Transaction Test ‚Üí {'age': 45, 'salary': 50000, 'transaction_amount': 3000} ‚Üí üß† Prediction: Fraudulent\n",
            "\n",
            "üö¶ Simulating Real-Time Transaction Stream:\n",
            "\n",
            "‚û°Ô∏è {'age': 28, 'salary': 25000, 'transaction_amount': 1200} ‚Üí üß† Prediction: Legit\n",
            "‚û°Ô∏è {'age': 52, 'salary': 70000, 'transaction_amount': 5000} ‚Üí üß† Prediction: Fraudulent\n",
            "‚û°Ô∏è {'age': 33, 'salary': 39000, 'transaction_amount': 1800} ‚Üí üß† Prediction: Legit\n",
            "\n",
            "‚úÖ Real-Time Streaming Simulation Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile fraud_api.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model and scaler\n",
        "model = joblib.load(\"fraud_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# Define input schema\n",
        "class Transaction(BaseModel):\n",
        "    age: int\n",
        "    salary: float\n",
        "    transaction_amount: float\n",
        "\n",
        "# Predict route\n",
        "@app.post(\"/predict\")\n",
        "def predict(data: Transaction):\n",
        "    input_df = pd.DataFrame([data.dict()])\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "    result = \"Fraudulent\" if prediction == 1 else \"Legit\"\n",
        "    return {\"prediction\": result}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2Pp_xHqRf9u",
        "outputId": "dfbc8b2f-dfaf-4af6-aefc-57f6b6e9edae"
      },
      "id": "u2Pp_xHqRf9u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting fraud_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok joblib pandas scikit-learn -q"
      ],
      "metadata": {
        "id": "JD7Pt3DNRqFN"
      },
      "id": "JD7Pt3DNRqFN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import conf\n",
        "conf.get_default().auth_token = \"309Gv8gorHu8mS2ztEWyKIJf3vU_3Md13NWpaeLA9gSkTbe6t\""
      ],
      "metadata": {
        "id": "oYqFU6Q1StFr"
      },
      "id": "oYqFU6Q1StFr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start public tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"üîó Public FastAPI URL: {public_url}\")\n",
        "\n",
        "# Run FastAPI app\n",
        "uvicorn.run(\"fraud_api:app\", host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX5im34vTBPu",
        "outputId": "a10e9ff7-407d-47a5-fdfc-e1868fc2dd53"
      },
      "id": "NX5im34vTBPu",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Public FastAPI URL: NgrokTunnel: \"https://a5b0bd4839c6.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [16072]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     105.112.178.162:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     105.112.178.162:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     105.112.178.162:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     105.112.178.162:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     105.112.178.162:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "fastapi\n",
        "uvicorn\n",
        "scikit-learn\n",
        "pandas\n",
        "joblib"
      ],
      "metadata": {
        "id": "N0O9OZwoUug5"
      },
      "id": "N0O9OZwoUug5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "# Use a lightweight Python image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements and install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the FastAPI app and model files\n",
        "COPY fraud_api.py .\n",
        "COPY fraud_model.pkl .\n",
        "COPY scaler.pkl .\n",
        "\n",
        "# Expose the port FastAPI will run on\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run the app using uvicorn\n",
        "CMD [\"uvicorn\", \"fraud_api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ],
      "metadata": {
        "id": "i6RPJ-VcU3Tl"
      },
      "id": "i6RPJ-VcU3Tl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üê≥ Docker Containerization\n",
        "\n",
        "To make the FastAPI fraud detection app portable and ready for deployment, we containerized it using Docker.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Dockerfile\n",
        "\n",
        "```Dockerfile\n",
        "# Use a lightweight Python image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy requirements and install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the FastAPI app and model files\n",
        "COPY fraud_api.py .\n",
        "COPY fraud_model.pkl .\n",
        "COPY scaler.pkl .\n",
        "\n",
        "# Expose the port FastAPI will run on\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run the app using uvicorn\n",
        "CMD [\"uvicorn\", \"fraud_api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇÔ∏è Project Structure\n",
        "\n",
        "```\n",
        "project/\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ fraud_api.py               # FastAPI prediction API  \n",
        "‚îú‚îÄ‚îÄ fraud_model.pkl            # Trained logistic regression model  \n",
        "‚îú‚îÄ‚îÄ scaler.pkl                 # Feature scaler  \n",
        "‚îú‚îÄ‚îÄ requirements.txt           # App dependencies  \n",
        "‚îî‚îÄ‚îÄ Dockerfile                 # Docker build instructions  \n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ How to Build and Run the Docker Container\n",
        "\n",
        "#### 1. Build the Docker image\n",
        "\n",
        "```bash\n",
        "docker build -t fraud-api .\n",
        "```\n",
        "\n",
        "#### 2. Run the Docker container\n",
        "\n",
        "```bash\n",
        "docker run -d -p 8000:8000 fraud-api\n",
        "```\n",
        "\n",
        "#### 3. Access the FastAPI documentation\n",
        "\n",
        "Go to: [http://localhost:8000/docs](http://localhost:8000/docs)\n",
        "\n",
        "You can now test the `/predict` endpoint using the Swagger UI."
      ],
      "metadata": {
        "id": "qgz03nV7VC_f"
      },
      "id": "qgz03nV7VC_f"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "# Load model and scaler\n",
        "model = joblib.load(\"fraud_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "# Sample stream of transactions\n",
        "transaction_stream = [\n",
        "    {\"age\": 25, \"salary\": 30000, \"transaction_amount\": 800},\n",
        "    {\"age\": 45, \"salary\": 50000, \"transaction_amount\": 3000},\n",
        "    {\"age\": 38, \"salary\": 45000, \"transaction_amount\": 1500},\n",
        "    {\"age\": 52, \"salary\": 70000, \"transaction_amount\": 5200},\n",
        "]\n",
        "\n",
        "print(\"üöÄ Simulating real-time transaction stream...\\n\")\n",
        "\n",
        "for txn in transaction_stream:\n",
        "    input_df = pd.DataFrame([txn])\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "    result = \"Fraudulent\" if prediction == 1 else \"Legit\"\n",
        "    print(f\"‚û°Ô∏è {txn} ‚Üí üß† Prediction: {result}\")\n",
        "    time.sleep(0.5)  # simulate slight delay\n",
        "\n",
        "print(\"\\n‚úÖ Streaming simulation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9quCBqkgpXk",
        "outputId": "e6dc538f-754b-463b-b109-8a3ec17277e3"
      },
      "id": "c9quCBqkgpXk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Simulating real-time transaction stream...\n",
            "\n",
            "‚û°Ô∏è {'age': 25, 'salary': 30000, 'transaction_amount': 800} ‚Üí üß† Prediction: Legit\n",
            "‚û°Ô∏è {'age': 45, 'salary': 50000, 'transaction_amount': 3000} ‚Üí üß† Prediction: Fraudulent\n",
            "‚û°Ô∏è {'age': 38, 'salary': 45000, 'transaction_amount': 1500} ‚Üí üß† Prediction: Legit\n",
            "‚û°Ô∏è {'age': 52, 'salary': 70000, 'transaction_amount': 5200} ‚Üí üß† Prediction: Fraudulent\n",
            "\n",
            "‚úÖ Streaming simulation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Real-Time Streaming Simulation\n",
        "\n",
        "To simulate real-time data streaming like Kafka, we used a Python loop with a time delay to mimic incoming transactions.\n",
        "\n",
        "Each transaction is passed through the trained model using the same preprocessing pipeline (scaler + logistic regression) and is instantly classified as either **Fraudulent** or **Legit**.\n",
        "\n",
        "### Sample Code\n",
        "\n",
        "```python\n",
        "import time\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "model = joblib.load(\"fraud_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "transaction_stream = [\n",
        "    {\"age\": 25, \"salary\": 30000, \"transaction_amount\": 800},\n",
        "    {\"age\": 45, \"salary\": 50000, \"transaction_amount\": 3000},\n",
        "    {\"age\": 38, \"salary\": 45000, \"transaction_amount\": 1500},\n",
        "    {\"age\": 52, \"salary\": 70000, \"transaction_amount\": 5200},\n",
        "]\n",
        "\n",
        "print(\"üöÄ Simulating real-time transaction stream...\\n\")\n",
        "\n",
        "for txn in transaction_stream:\n",
        "    input_df = pd.DataFrame([txn])\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "    result = \"Fraudulent\" if prediction == 1 else \"Legit\"\n",
        "    print(f\"‚û°Ô∏è {txn} ‚Üí üß† Prediction: {result}\")\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\n‚úÖ Streaming simulation complete.\")"
      ],
      "metadata": {
        "id": "Z5ejsnoShVP8"
      },
      "id": "Z5ejsnoShVP8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}