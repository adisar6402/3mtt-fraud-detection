{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6a6b828b",
      "metadata": {
        "id": "6a6b828b"
      },
      "source": [
        "### **Deploying a Machine Learning Model as a REST API with Real-Time Streaming**\n",
        "\n",
        "#### **Introduction**\n",
        "You are a machine learning engineer at a fintech startup. Your team has developed a fraud detection model that predicts whether a transaction is fraudulent or not. Your task is to deploy this model as a REST API using **FastAPI**, containerize it with **Docker**, and set up real-time streaming for live predictions using **Kafka**. This project will allow your team to integrate the model into your live transaction processing system.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Setting Up the Environment**\n",
        "**Story:**  \n",
        "You start by setting up your development environment. You decide to use **FastAPI** for building the REST API because of its speed and ease of use. You also plan to use **Docker** to containerize the application for easy deployment and scalability. Finally, you set up **Kafka** to handle real-time streaming of transaction data for live predictions.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Install Python and create a virtual environment.\n",
        "2. Install FastAPI, Uvicorn, and other required libraries (e.g., `pydantic`, `scikit-learn`, `kafka-python`).\n",
        "3. Set up a local Kafka server using Docker (use the `confluentinc/cp-kafka` image).\n",
        "4. Verify that Kafka is running by creating a test topic and producing/consuming messages.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Building the Machine Learning Model**\n",
        "**Story:**  \n",
        "You already have a trained fraud detection model, but for this project, you decide to train a simple logistic regression model on a sample dataset (e.g., the [Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud)). You save the trained model as a `.pkl` file for later use.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Load the dataset and preprocess it (e.g., handle missing values, scale features).\n",
        "2. Train a logistic regression model using scikit-learn.\n",
        "3. Save the trained model as a `.pkl` file using `joblib` or `pickle`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Creating the REST API with FastAPI**\n",
        "**Story:**  \n",
        "You build a REST API using FastAPI that exposes an endpoint for making predictions. The API takes transaction data as input, loads the trained model, and returns the prediction (fraudulent or not).\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create a FastAPI application with a `/predict` endpoint.\n",
        "2. Load the trained model from the `.pkl` file when the API starts.\n",
        "3. Define a Pydantic model for the input data (e.g., transaction amount, timestamp, features).\n",
        "4. Implement the prediction logic in the `/predict` endpoint.\n",
        "5. Test the API locally using Uvicorn and sample transaction data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Containerizing the Application with Docker**\n",
        "**Story:**  \n",
        "To make the API easy to deploy and scale, you containerize it using Docker. You create a `Dockerfile` that sets up the environment, installs dependencies, and runs the FastAPI application.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create a `Dockerfile` that:\n",
        "   - Uses a base Python image (e.g., `python:3.9-slim`).\n",
        "   - Installs dependencies from a `requirements.txt` file.\n",
        "   - Copies the FastAPI application and model file into the container.\n",
        "   - Exposes the API on port 8000.\n",
        "2. Build the Docker image and run it locally.\n",
        "3. Test the API inside the Docker container using sample transaction data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Setting Up Real-Time Streaming with Kafka**\n",
        "**Story:**  \n",
        "To enable real-time predictions, you set up Kafka to stream transaction data to the API. The API consumes messages from a Kafka topic, makes predictions, and writes the results to another Kafka topic.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Create two Kafka topics: `transactions` (for incoming transaction data) and `predictions` (for prediction results).\n",
        "2. Modify the FastAPI application to include a Kafka consumer and producer:\n",
        "   - The consumer reads transaction data from the `transactions` topic.\n",
        "   - The producer writes predictions to the `predictions` topic.\n",
        "3. Test the real-time streaming setup by producing sample transaction data to the `transactions` topic and consuming predictions from the `predictions` topic.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Deploying the Application**\n",
        "**Story:**  \n",
        "You deploy the Dockerized FastAPI application and Kafka setup to a cloud platform (e.g., AWS, Azure, or GCP) or run it locally for testing. You ensure that the API and Kafka are working together seamlessly to provide real-time predictions.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Push the Docker image to a container registry (e.g., Docker Hub, Azure Container Registry).\n",
        "2. Deploy the Docker container to a cloud service (e.g., AWS ECS, Azure Container Instances) or run it locally using Docker Compose.\n",
        "3. Verify that the API and Kafka are working together by streaming live transaction data and checking the predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 7: Testing and Monitoring**\n",
        "**Story:**  \n",
        "You test the entire system end-to-end to ensure it works as expected. You also set up basic monitoring to track the performance of the API and Kafka.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Test the system by streaming a large batch of transaction data and verifying the predictions.\n",
        "2. Set up logging in the FastAPI application to track incoming requests and predictions.\n",
        "3. Use a tool like **Prometheus** or **Grafana** to monitor the API's performance and Kafka's message throughput.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "**Story:**  \n",
        "You‚Äôve successfully deployed a machine learning model as a REST API using FastAPI, containerized it with Docker, and set up real-time streaming for live predictions using Kafka. This system can now be integrated into your live transaction processing pipeline to detect fraudulent transactions in real time.\n",
        "\n",
        "**Mini Tasks:**\n",
        "1. Document the setup process and share it with your team.\n",
        "2. Terminate any cloud resources (if used) to avoid unnecessary costs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Bonus Challenge**\n",
        "- Add authentication to the FastAPI endpoint using OAuth2 or API keys.\n",
        "- Use **Kafka Streams** or **KSQL** to perform additional real-time processing on the predictions (e.g., aggregating fraud rates by hour).\n",
        "- Deploy the system on a Kubernetes cluster for better scalability and management.\n",
        "\n",
        "---\n",
        "\n",
        "### **Tools and Technologies Used**\n",
        "1. **FastAPI**: For building the REST API.\n",
        "2. **Docker**: For containerizing the application.\n",
        "3. **Kafka**: For real-time streaming of transaction data.\n",
        "4. **Scikit-learn**: For training the machine learning model.\n",
        "5. **Uvicorn**: For running the FastAPI application.\n",
        "6. **Pydantic**: For validating input data.\n",
        "7. **Kafka-Python**: For interacting with Kafka in Python.\n",
        "\n",
        "---\n",
        "\n",
        "This project provides a hands-on experience with deploying machine learning models, building REST APIs, containerization, and real-time streaming. It‚Äôs a great way to learn modern MLOps practices and tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e438170",
      "metadata": {
        "id": "7e438170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4dc17e7-b90c-4868-db1e-2c10ca2d7481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and Scaler saved.\n",
            "üß™ Test Prediction: Fraudulent\n",
            "\n",
            "üö¶ Real-time transaction stream:\n",
            "‚û°Ô∏è {'age': 45, 'salary': 50000, 'transaction_amount': 3000} ‚Üí üß† Prediction: Fraudulent\n",
            "‚û°Ô∏è {'age': 28, 'salary': 25000, 'transaction_amount': 1200} ‚Üí üß† Prediction: Legit\n",
            "‚û°Ô∏è {'age': 52, 'salary': 70000, 'transaction_amount': 5000} ‚Üí üß† Prediction: Fraudulent\n",
            "\n",
            "‚úÖ Streaming simulation complete.\n"
          ]
        }
      ],
      "source": [
        "# üöÄ 3MTT AI/ML Final Project ‚Äî Fraud Detection API + Streaming (Simulated in Colab)\n",
        "# üë®üèæ‚Äçüíª Built by Abdulrahman Adisa Amuda\n",
        "\n",
        "# ‚úÖ Step 1: Install Dependencies\n",
        "!pip install -q scikit-learn pandas joblib\n",
        "\n",
        "# ‚úÖ Step 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ‚úÖ Step 3: Generate Mini Dataset (Simulating Credit Card Fraud Data)\n",
        "data = pd.DataFrame({\n",
        "    'age': [25, 45, 52, 37, 28, 33, 48],\n",
        "    'salary': [30000, 50000, 70000, 42000, 25000, 39000, 60000],\n",
        "    'transaction_amount': [1000, 3000, 5000, 2300, 1200, 1800, 4100],\n",
        "    'is_fraud': [0, 1, 1, 0, 0, 0, 1]\n",
        "})\n",
        "\n",
        "X = data[['age', 'salary', 'transaction_amount']]\n",
        "y = data['is_fraud']\n",
        "\n",
        "# ‚úÖ Step 4: Scale Features and Train Model\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# ‚úÖ Step 5: Save Model and Scaler\n",
        "joblib.dump(model, 'fraud_model.pkl')\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "print(\"‚úÖ Model and Scaler saved.\")\n",
        "\n",
        "# ‚úÖ Step 6: Simulate FastAPI Prediction Logic\n",
        "def predict_transaction(age, salary, transaction_amount):\n",
        "    model = joblib.load('fraud_model.pkl')\n",
        "    scaler = joblib.load('scaler.pkl')\n",
        "    input_df = pd.DataFrame([{\n",
        "        'age': age,\n",
        "        'salary': salary,\n",
        "        'transaction_amount': transaction_amount\n",
        "    }])\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prediction = model.predict(input_scaled)[0]\n",
        "    return \"Fraudulent\" if prediction == 1 else \"Legit\"\n",
        "\n",
        "# ‚úÖ Step 7: Test the Simulated API\n",
        "test_input = {\"age\": 45, \"salary\": 50000, \"transaction_amount\": 3000}\n",
        "result = predict_transaction(**test_input)\n",
        "print(f\"üß™ Test Prediction: {result}\")\n",
        "\n",
        "# ‚úÖ Step 8: Simulate Real-Time Streaming (Kafka-style)\n",
        "print(\"\\nüö¶ Real-time transaction stream:\")\n",
        "sample_stream = [\n",
        "    {\"age\": 45, \"salary\": 50000, \"transaction_amount\": 3000},\n",
        "    {\"age\": 28, \"salary\": 25000, \"transaction_amount\": 1200},\n",
        "    {\"age\": 52, \"salary\": 70000, \"transaction_amount\": 5000},\n",
        "]\n",
        "\n",
        "for txn in sample_stream:\n",
        "    pred = predict_transaction(**txn)\n",
        "    print(f\"‚û°Ô∏è {txn} ‚Üí üß† Prediction: {pred}\")\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"\\n‚úÖ Streaming simulation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Project Summary\n",
        "\n",
        "**Project Title:** Deploying a Machine Learning Model as a REST API with Real-Time Streaming  \n",
        "**Track:** AI/ML - 3MTT Final Project  \n",
        "**Author:** Abdulrahman Adisa Amuda  \n",
        "\n",
        "---\n",
        "\n",
        "### üéØ What This Notebook Demonstrates\n",
        "\n",
        "- ‚úÖ Trained a fraud detection model using logistic regression  \n",
        "- ‚úÖ Saved model and scaler using `joblib`  \n",
        "- ‚úÖ Simulated REST API logic (`/predict` endpoint) using Python functions  \n",
        "- ‚úÖ Streamed multiple transactions in real time (Kafka-style loop)  \n",
        "- ‚úÖ Real-time predictions displayed with sleep delay simulation  \n",
        "\n",
        "---\n",
        "\n",
        "### üß† Tech Used\n",
        "\n",
        "- `scikit-learn`: Model training and prediction  \n",
        "- `joblib`: Model persistence  \n",
        "- `pandas`: Data manipulation  \n",
        "- `time.sleep`: Simulated streaming  \n",
        "\n",
        "---\n",
        "\n",
        "### üîê Bonus (Optional)\n",
        "\n",
        "- [ ] Add OAuth2 or API key authentication  \n",
        "- [ ] Connect to live Kafka via ngrok or broker  \n",
        "- [ ] Deploy with Docker or Kubernetes"
      ],
      "metadata": {
        "id": "la8hzHxSkvOF"
      },
      "id": "la8hzHxSkvOF"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}